{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4McSM0i2dIvu",
        "RZzQcrZpgpPj",
        "17lDdv9CWHxr",
        "wabr6JZTNDkm",
        "OKPvU4o7crkT",
        "XkWeuCSx-7PP"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thisisindrajit/Using-Deep-CNN-to-enhance-low-res-photos-to-match-the-quality-of-DSLR-cameras/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzTPgTQk0oBm"
      },
      "source": [
        "# References and citation\n",
        "\n",
        "This notebook contains references for code from this repository: https://github.com/aiff22/DPED.git\n",
        "\n",
        "**This project is for educational purposes only.**\n",
        "\n",
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "\n",
        "Citation:\n",
        "\n",
        "<pre>\n",
        "@inproceedings{ignatov2017dslr,\n",
        "  title={DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks},\n",
        "  author={Ignatov, Andrey and Kobyshev, Nikolay and Timofte, Radu and Vanhoey, Kenneth and Van Gool, Luc},\n",
        "  booktitle={Proceedings of the IEEE International Conference on Computer Vision},\n",
        "  pages={3277--3285},\n",
        "  year={2017}\n",
        "}\n",
        "</pre>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yladDaJktiLA"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJJ2_urdX5Zr",
        "outputId": "ee435134-5af9-425e-d240-bd71c9d02c0f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZKJju9WWcT4",
        "outputId": "2e024103-ef33-4636-ba96-08f0bc4a108a"
      },
      "source": [
        "#%tensorflow_version 1.x\n",
        "!pip install -U tensorflow-addons\n",
        "\n",
        "import numpy as np\n",
        "import imageio\n",
        "import PIL #pillow\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from __future__ import print_function\n",
        "import scipy\n",
        "import scipy.io\n",
        "import os\n",
        "import sys\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from scipy.ndimage.filters import convolve\n",
        "from PIL import Image\n",
        "from functools import reduce\n",
        "\n",
        "#tensorflow version\n",
        "print(tf.__version__)\n",
        "\n",
        "tf.compat.v1.disable_v2_behavior() #to disable tensorflow v2 behavior"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\r\u001b[K     |▌                               | 10kB 25.0MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 31.5MB/s eta 0:00:01\r\u001b[K     |█▍                              | 30kB 21.1MB/s eta 0:00:01\r\u001b[K     |█▉                              | 40kB 19.3MB/s eta 0:00:01\r\u001b[K     |██▎                             | 51kB 21.6MB/s eta 0:00:01\r\u001b[K     |██▉                             | 61kB 15.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 71kB 15.9MB/s eta 0:00:01\r\u001b[K     |███▊                            | 81kB 15.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 92kB 15.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 102kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 112kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 122kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 133kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 143kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 153kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 163kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 174kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 184kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 194kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 204kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 215kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 225kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 235kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 245kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 256kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 266kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 276kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 286kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 296kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 307kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 317kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 327kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 337kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 348kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 358kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 368kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 378kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 389kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 399kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 409kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 419kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 430kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 440kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 450kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 460kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 471kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 481kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 491kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 501kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 512kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 522kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 532kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 542kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 552kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 563kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 573kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 583kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 593kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 604kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 614kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 624kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 634kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 645kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 655kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 665kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 675kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 686kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 696kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 706kB 15.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.12.1\n",
            "2.4.1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCs83nMYeyD2"
      },
      "source": [
        "# Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aeUrRZze3GW"
      },
      "source": [
        "phone = \"iphone\"\n",
        "dped_dir = \"drive/MyDrive/Innovation_Practices_Lab/dped_small/\"\n",
        "\n",
        "#arguments for training\n",
        "train_size = 6000 #the number of training patches randomly loaded each eval_step iterations\n",
        "learning_rate = 5e-4\n",
        "num_train_iters = 10000\n",
        "w_content = 10\n",
        "w_color = 0.5\n",
        "w_texture = 1\n",
        "w_tv = 2000\n",
        "vgg_dir = \"drive/MyDrive/Innovation_Practices_Lab/vgg_pretrained/imagenet-vgg-verydeep-19.mat\"\n",
        "eval_step = 1000\n",
        "PATCH_HEIGHT = 100\n",
        "PATCH_WIDTH = 100\n",
        "PATCH_SIZE = PATCH_WIDTH * PATCH_HEIGHT * 3\n",
        "batch_size = 50\n",
        "\n",
        "#arguments for testing\n",
        "test_subset = \"all\"\n",
        "resolution = \"orig\"\n",
        "\n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t57cD3autEfO"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6zkVsAmtOly"
      },
      "source": [
        "#helper functions to load batch training data and test data\n",
        "\n",
        "#function to load test data\n",
        "def load_test_data(phone, dped_dir, IMAGE_SIZE):\n",
        "\n",
        "    #print(phone + \" \" + dped_dir + \" \" + str(IMAGE_SIZE))\n",
        "    \n",
        "    test_directory_phone = dped_dir + str(phone) + '/test_data/patches/' + str(phone) + '/' #input\n",
        "    test_directory_dslr = dped_dir + str(phone) + '/test_data/patches/canon/' #target output\n",
        "\n",
        "    #finding no of test images\n",
        "    NUM_TEST_IMAGES = len([name for name in os.listdir(test_directory_phone)\n",
        "                           if os.path.isfile(os.path.join(test_directory_phone, name))])\n",
        "\n",
        "    #empty arrays with no of rows = number of test images and no of columns = 100*100*3\n",
        "    test_data = np.zeros((NUM_TEST_IMAGES, IMAGE_SIZE))\n",
        "    test_answ = np.zeros((NUM_TEST_IMAGES, IMAGE_SIZE))\n",
        "\n",
        "    for i in range(0,NUM_TEST_IMAGES):\n",
        "        \n",
        "        I = np.asarray(imageio.imread(test_directory_phone + str(i) + '.jpg'))\n",
        "        I = np.float16(np.reshape(I, [1, IMAGE_SIZE]))/255\n",
        "        test_data[i, :] = I\n",
        "        \n",
        "        I = np.asarray(imageio.imread(test_directory_dslr + str(i) + '.jpg'))\n",
        "        I = np.float16(np.reshape(I, [1, IMAGE_SIZE]))/255\n",
        "        test_answ[i, :] = I\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(str(round(i * 100 / NUM_TEST_IMAGES)) + \"% done\")\n",
        "\n",
        "    return test_data, test_answ\n",
        "\n",
        "#function to load training data\n",
        "def load_batch(phone, dped_dir, TRAIN_SIZE, IMAGE_SIZE):\n",
        "\n",
        "    #print(phone + \" \" + dped_dir + \" \" + IMAGE_SIZE)\n",
        "\n",
        "    train_directory_phone = dped_dir + str(phone) + '/training_data/' + str(phone) + '/'\n",
        "    train_directory_dslr = dped_dir + str(phone) + '/training_data/canon/'\n",
        "\n",
        "    #finding no of training images\n",
        "    NUM_TRAINING_IMAGES = len([name for name in os.listdir(train_directory_phone)\n",
        "                               if os.path.isfile(os.path.join(train_directory_phone, name))])\n",
        "\n",
        "    #if TRAIN_SIZE == -1 then load all images\n",
        "    if TRAIN_SIZE == -1:\n",
        "        TRAIN_SIZE = NUM_TRAINING_IMAGES\n",
        "        #here np.arange returns an array with values from 0 to TRAIN_SIZE-1 -> [0,1,2,3.....]\n",
        "        TRAIN_IMAGES = np.arange(0, TRAIN_SIZE)\n",
        "    else:\n",
        "        #randomly selecting TRAIN_SIZE no of images from total no of training images\n",
        "        TRAIN_IMAGES = np.random.choice(np.arange(0, NUM_TRAINING_IMAGES-1), TRAIN_SIZE, replace=False)\n",
        "\n",
        "    #empty arrays with no of rows = number of training images and no of columns = 100*100*3\n",
        "    train_data = np.zeros((TRAIN_SIZE, IMAGE_SIZE))\n",
        "    train_answ = np.zeros((TRAIN_SIZE, IMAGE_SIZE))\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    for img in TRAIN_IMAGES:\n",
        "\n",
        "        I = np.asarray(imageio.imread(train_directory_phone + str(img) + '.jpg'))\n",
        "        I = np.float16(np.reshape(I, [1, IMAGE_SIZE])) / 255\n",
        "        train_data[i, :] = I\n",
        "\n",
        "        I = np.asarray(imageio.imread(train_directory_dslr + str(img) + '.jpg'))\n",
        "        I = np.float16(np.reshape(I, [1, IMAGE_SIZE])) / 255\n",
        "        train_answ[i, :] = I\n",
        "\n",
        "        i += 1\n",
        "        if i % 100 == 0:\n",
        "            print(str(round(i * 100 / TRAIN_SIZE)) + \"% done\")\n",
        "\n",
        "    return train_data, train_answ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J36Fk2qscbVE"
      },
      "source": [
        "# VGG 19 pretrained model (conversion of MATLAB file to Neural Network)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P76eGrcckBD"
      },
      "source": [
        "#helper functions for vgg 19 conversion\n",
        "\n",
        "def conv_layer_vgg(input, weights, bias):\n",
        "  conv = tf.nn.conv2d(input, tf.constant(weights), strides=(1, 1, 1, 1), padding='SAME')\n",
        "  return tf.nn.bias_add(conv, bias)\n",
        "\n",
        "def pool_layer_vgg(input):\n",
        "  return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding='SAME')\n",
        "\n",
        "def preprocess(image):\n",
        "  return image - IMAGE_MEAN\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "IMAGE_MEAN = np.array([123.68 ,  116.779,  103.939])\n",
        "\n",
        "#conversion of matlab file to neural network\n",
        "def net(path_to_vgg_net, input_image):\n",
        "\n",
        "  layers = (\n",
        "      'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
        "\n",
        "      'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
        "\n",
        "      'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n",
        "      'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
        "\n",
        "      'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n",
        "      'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
        "\n",
        "      'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n",
        "      'relu5_3', 'conv5_4', 'relu5_4'\n",
        "  )\n",
        "\n",
        "  data = scipy.io.loadmat(path_to_vgg_net)\n",
        "  weights = data['layers'][0]\n",
        "\n",
        "  net = {}\n",
        "\n",
        "  current = input_image\n",
        "  \n",
        "  for i, name in enumerate(layers):\n",
        "      layer_type = name[:4]\n",
        "      \n",
        "      if layer_type == 'conv':\n",
        "          kernels, bias = weights[i][0][0][0][0]\n",
        "          kernels = np.transpose(kernels, (1, 0, 2, 3))\n",
        "          bias = bias.reshape(-1)\n",
        "          current = conv_layer_vgg(current, kernels, bias)\n",
        "\n",
        "      elif layer_type == 'relu':\n",
        "          current = tf.nn.relu(current)\n",
        "\n",
        "      elif layer_type == 'pool':\n",
        "          current = pool_layer_vgg(current)\n",
        "  \n",
        "      net[name] = current\n",
        "\n",
        "  return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFJA4LyrKmn_"
      },
      "source": [
        "# Helper functions for models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVYPu_kmKyOd"
      },
      "source": [
        "#helper function to define the weight variable\n",
        "def weight(shape, name):\n",
        "\n",
        "  weight_var = tf.compat.v1.truncated_normal(shape, stddev=0.01)  \n",
        "  return tf.Variable(weight_var, name=name)\n",
        "\n",
        "#helper function to define the bias variable\n",
        "def bias(shape, name):\n",
        "\n",
        "  bias_var = tf.constant(0.01, shape=shape)\n",
        "  return tf.Variable(bias_var, name=name)\n",
        "\n",
        "#helper function to create a 2d convolutional layer\n",
        "def conv2d(x, W):\n",
        "  #tf.nn.conv2d computes a 2-D convolution given a input and 4-D tensor having various values related to the kernel/filter. \n",
        "  #x is the input, W is the parameter with shape [filter_height, filter_width, in_channels, out_channels]\n",
        "  #padding='SAME' means the output must have the same height/width dimension as the input.\n",
        "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "#helper function to perform Batch Normalization\n",
        "#reference for batch normalization - https://towardsdatascience.com/batch-normalization-explained-algorithm-breakdown-23d2794511c\n",
        "def batch_normalization(convnet):\n",
        "\n",
        "  #here i.value must be present for sure ####VERY IMPORTANT\n",
        "  batch, rows, cols, channels = [i.value for i in convnet.get_shape()]\n",
        "  #batch, rows, cols, channels = [i for i in convnet.get_shape()]\n",
        "  var_shape = [channels]\n",
        "\n",
        "  mean, variance = tf.compat.v1.nn.moments(convnet, [1,2], keepdims=True)\n",
        "  shift = tf.Variable(tf.zeros(var_shape))\n",
        "  scale = tf.Variable(tf.ones(var_shape))\n",
        "\n",
        "  epsilon = 1e-3\n",
        "  normalized = (convnet-mean)/(variance + epsilon)**(.5)\n",
        "\n",
        "  return scale * normalized + shift\n",
        "\n",
        "\n",
        "#helper function to create a convolutional layer with Leaky ReLU activation function\n",
        "def convolutionallayerwithLeakyReLU(net, num_filters, filter_size, strides, batch_nn=True):\n",
        "    \n",
        "  weights_init = initialize_vars(net, num_filters, filter_size)\n",
        "  strides_shape = [1, strides, strides, 1] #NHWC format - N, Height, Width, Channels. For the most common case of the same horizontal and vertical strides, strides = [1, stride, stride, 1].\n",
        "  bias = tf.Variable(tf.constant(0.01, shape=[num_filters]))\n",
        "\n",
        "  #here weights_init is nothing but the parameters for filter with shape [filter_height, filter_width, in_channels, out_channels]\n",
        "  net = tf.nn.conv2d(net, weights_init, strides_shape, padding='SAME') + bias   \n",
        "  net = leakyReLU(net)\n",
        "\n",
        "  #if batch normalization needs to be done\n",
        "  if batch_nn:\n",
        "       net = batch_normalization(net)\n",
        "\n",
        "  return net\n",
        "\n",
        "#helper function to initialize variables before creating a convolutional layer\n",
        "def initialize_vars(net, out_channels, filter_size, transpose=False):\n",
        "\n",
        "  #here i.value must be present for sure ####VERY IMPORTANT\n",
        "  _, rows, cols, in_channels = [i.value for i in net.get_shape()] \n",
        "\n",
        "  #_, rows, cols, in_channels = [i for i in net.get_shape()]\n",
        "\n",
        "  if not transpose:\n",
        "    weights_shape = [filter_size, filter_size, in_channels, out_channels]\n",
        "  else:\n",
        "    weights_shape = [filter_size, filter_size, out_channels, in_channels]\n",
        "\n",
        "  #check this out\n",
        "  weights_init = tf.Variable(tf.compat.v1.truncated_normal(weights_shape, stddev=0.01, seed=1), dtype=tf.float32)\n",
        "  \n",
        "  return weights_init\n",
        "\n",
        "#Leaky ReLU function\n",
        "def leakyReLU(x, alpha = 0.2):\n",
        "  return tf.maximum(alpha * x, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpF6OwXfgiMR"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU4Ya1uWgsnC"
      },
      "source": [
        "#generator network - returns a enhanced version of the given input image\n",
        "def residualnetwork(input_image):\n",
        "\n",
        "  #this is used to store all the variables related to generator as a whole by tensorflow\n",
        "  with tf.compat.v1.variable_scope(\"generator\"):\n",
        "\n",
        "    #first layer (layer which has input image as the layer input)\n",
        "    W1 = weight([9,9,3,64], name=\"W1\")\n",
        "    b1 = bias([64], name=\"b1\")\n",
        "\n",
        "    #ReLU activation of sum of conv2d neural network weight outputs and bias\n",
        "    c1 = tf.keras.activations.relu(conv2d(input_image, W1) + b1)\n",
        "\n",
        "    #residual block 1\n",
        "\n",
        "    W2 = weight([3, 3, 64, 64], name=\"W2\")\n",
        "    b2 = bias([64], name=\"b2\")\n",
        "\n",
        "    c2 = tf.keras.activations.relu(batch_normalization(conv2d(c1, W2) + b2))\n",
        "\n",
        "    W3 = weight([3, 3, 64, 64], name=\"W3\")\n",
        "    b3 = bias([64], name=\"b3\");\n",
        "    \n",
        "    c3 = tf.keras.activations.relu(batch_normalization(conv2d(c2, W3) + b3)) + c1 #here we add c1 because it is a residual neural network\n",
        "\n",
        "    #residual block 2\n",
        "\n",
        "    W4 = weight([3, 3, 64, 64], name=\"W4\")\n",
        "    b4 = bias([64], name=\"b4\")\n",
        "\n",
        "    c4 = tf.keras.activations.relu(batch_normalization(conv2d(c3, W4) + b4))\n",
        "\n",
        "    W5 = weight([3, 3, 64, 64], name=\"W5\")\n",
        "    b5 = bias([64], name=\"b5\");\n",
        "\n",
        "    c5 = tf.keras.activations.relu(batch_normalization(conv2d(c4, W5) + b5)) + c3\n",
        "\n",
        "    #residual block 3\n",
        "\n",
        "    W6 = weight([3, 3, 64, 64], name=\"W6\")\n",
        "    b6 = bias([64], name=\"b6\")\n",
        "\n",
        "    c6 = tf.keras.activations.relu(batch_normalization(conv2d(c5, W6) + b6))\n",
        "\n",
        "    W7 = weight([3, 3, 64, 64], name=\"W7\")\n",
        "    b7 = bias([64], name=\"b7\");\n",
        "    \n",
        "    c7 = tf.keras.activations.relu(batch_normalization(conv2d(c6, W7) + b7)) + c5\n",
        "\n",
        "    #residual block 4\n",
        "\n",
        "    W8 = weight([3, 3, 64, 64], name=\"W8\")\n",
        "    b8 = bias([64], name=\"b8\")\n",
        "\n",
        "    c8 = tf.keras.activations.relu(batch_normalization(conv2d(c7, W8) + b8))\n",
        "\n",
        "    W9 = weight([3, 3, 64, 64], name=\"W9\")\n",
        "    b9 = bias([64], name=\"b9\")\n",
        "\n",
        "    c9 = tf.keras.activations.relu(batch_normalization(conv2d(c8, W9) + b9)) + c7\n",
        "\n",
        "    #residual block 5 (extra)\n",
        "\n",
        "    W10 = weight([3, 3, 64, 64], name=\"W10\")\n",
        "    b10 = bias([64], name=\"b10\")\n",
        "\n",
        "    c10 = tf.keras.activations.relu(batch_normalization(conv2d(c9, W10) + b10))\n",
        "\n",
        "    W11 = weight([3, 3, 64, 64], name=\"W11\")\n",
        "    b11 = bias([64], name=\"b11\")\n",
        "\n",
        "    c11 = tf.keras.activations.relu(batch_normalization(conv2d(c10, W11) + b11)) + c9\n",
        "\n",
        "    #Additional convolutional layers\n",
        "\n",
        "    W12 = weight([3, 3, 64, 64], name=\"W12\")\n",
        "    b12 = bias([64], name=\"b12\")\n",
        "\n",
        "    c12 = tf.keras.activations.relu(conv2d(c11, W12) + b12)\n",
        "\n",
        "    W13 = weight([3, 3, 64, 256], name=\"W13\")\n",
        "    b13 = bias([256], name=\"b13\")\n",
        "\n",
        "    c13 = tf.keras.activations.relu(conv2d(c12, W13) + b13)\n",
        "\n",
        "    #Output layer\n",
        "\n",
        "    W14 = weight([9, 9, 256, 3], name=\"W14\") \n",
        "    b14 = bias([3], name=\"b14\");\n",
        "\n",
        "    #Since tanh(x) function is mapping from (-∞, +∞) to (0, 1), the network's output value should be large to get strictly 0 or 1. \n",
        "    #Using a wider interval allows to fix both boundaries small and constant.\n",
        "    enhanced = tf.keras.activations.tanh(conv2d(c13, W14) + b14) * 0.58 + 0.5\n",
        "\n",
        "\n",
        "  return enhanced\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "#discriminator network - produces a probability that the given image has been taken by a DSLR camera\n",
        "def adverserialnetwork(image):\n",
        "  \n",
        "  with tf.compat.v1.variable_scope(\"discriminator\"):\n",
        "    \n",
        "    conv1 = convolutionallayerwithLeakyReLU(image, 48, 11, 4, batch_nn = False) #48 filters, 11*11 filter size, 4 - stride\n",
        "    conv2 = convolutionallayerwithLeakyReLU(conv1, 128, 5, 2)\n",
        "    conv3 = convolutionallayerwithLeakyReLU(conv2, 192, 3, 1)\n",
        "    conv4 = convolutionallayerwithLeakyReLU(conv3, 192, 3, 1)\n",
        "    conv5 = convolutionallayerwithLeakyReLU(conv4, 128, 3, 2)\n",
        "\n",
        "    flat_size = 128 * 7 * 7\n",
        "\n",
        "    #reshapes a tensor to the given shape (second parameter is the given shape)\n",
        "    conv5_flat = tf.reshape(conv5, [-1, flat_size])\n",
        "\n",
        "    W_fc = tf.Variable(tf.compat.v1.truncated_normal([flat_size, 1024], stddev=0.01))\n",
        "    bias_fc = tf.Variable(tf.constant(0.01, shape=[1024]))\n",
        "\n",
        "    fc = leakyReLU(tf.matmul(conv5_flat, W_fc) + bias_fc) #vectorization\n",
        "\n",
        "    W_out = tf.Variable(tf.compat.v1.truncated_normal([1024, 2], stddev=0.01))\n",
        "    bias_out = tf.Variable(tf.constant(0.01, shape=[2]))\n",
        "\n",
        "    #softmax function - calculates the probabilities of each target class over all possible target classes and returns the probabilities. Similar to the sigmoid function but differs in its own way.\n",
        "    #reference - https://deepai.org/machine-learning-glossary-and-terms/softmax-layer\n",
        "    adv_out = tf.nn.softmax(tf.matmul(fc, W_out) + bias_out)\n",
        "    \n",
        "  return adv_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4McSM0i2dIvu"
      },
      "source": [
        "# SSIM helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zs4BD6GdQnH"
      },
      "source": [
        "#referred from the paper - DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks\n",
        "#link to paper - https://arxiv.org/pdf/1704.02470.pdf\n",
        "\n",
        "def _FSpecialGauss(size, sigma):\n",
        "\n",
        "  radius = size // 2\n",
        "  offset = 0.0\n",
        "  start, stop = -radius, radius + 1\n",
        "\n",
        "  if size % 2 == 0:\n",
        "    offset = 0.5\n",
        "    stop -= 1\n",
        "\n",
        "  x, y = np.mgrid[offset + start:stop, offset + start:stop]\n",
        "  g = np.exp(-((x**2 + y**2)/(2.0 * sigma**2)))\n",
        "\n",
        "  return g / g.sum()\n",
        "\n",
        "\n",
        "def _SSIMForMultiScale(img1, img2, max_val=255, filter_size=11, filter_sigma=1.5, k1=0.01, k2=0.03):\n",
        "\n",
        "  img1 = img1.astype(np.float64)\n",
        "  img2 = img2.astype(np.float64)\n",
        "  _, height, width, _ = img1.shape\n",
        "\n",
        "  size = min(filter_size, height, width)\n",
        "  sigma = size * filter_sigma / filter_size if filter_size else 0\n",
        "\n",
        "  if filter_size:\n",
        "\n",
        "    window = np.reshape(_FSpecialGauss(size, sigma), (1, size, size, 1))\n",
        "    mu1 = signal.fftconvolve(img1, window, mode='valid')\n",
        "    mu2 = signal.fftconvolve(img2, window, mode='valid')\n",
        "    sigma11 = signal.fftconvolve(img1 * img1, window, mode='valid')\n",
        "    sigma22 = signal.fftconvolve(img2 * img2, window, mode='valid')\n",
        "    sigma12 = signal.fftconvolve(img1 * img2, window, mode='valid')\n",
        "\n",
        "  else:\n",
        "\n",
        "    mu1, mu2 = img1, img2\n",
        "    sigma11 = img1 * img1\n",
        "    sigma22 = img2 * img2\n",
        "    sigma12 = img1 * img2\n",
        "\n",
        "  mu11 = mu1 * mu1\n",
        "  mu22 = mu2 * mu2\n",
        "  mu12 = mu1 * mu2\n",
        "  sigma11 -= mu11\n",
        "  sigma22 -= mu22\n",
        "  sigma12 -= mu12\n",
        "\n",
        "  c1 = (k1 * max_val) ** 2\n",
        "  c2 = (k2 * max_val) ** 2\n",
        "  v1 = 2.0 * sigma12 + c2\n",
        "  v2 = sigma11 + sigma22 + c2\n",
        "\n",
        "  ssim = np.mean((((2.0 * mu12 + c1) * v1) / ((mu11 + mu22 + c1) * v2)))\n",
        "  cs = np.mean(v1 / v2)\n",
        "\n",
        "  return ssim, cs\n",
        "\n",
        "\n",
        "def MultiScaleSSIM(img1, img2, max_val=255, filter_size=11, filter_sigma=1.5, k1=0.01, k2=0.03, weights=None):\n",
        "\n",
        "  weights = np.array(weights if weights else [0.0448, 0.2856, 0.3001, 0.2363, 0.1333])\n",
        "  levels = weights.size\n",
        "\n",
        "  downsample_filter = np.ones((1, 2, 2, 1)) / 4.0\n",
        "  im1, im2 = [x.astype(np.float64) for x in [img1, img2]]\n",
        "\n",
        "  mssim = np.array([])\n",
        "  mcs = np.array([])\n",
        "\n",
        "  for _ in range(levels):\n",
        "\n",
        "    ssim, cs = _SSIMForMultiScale(im1, im2, max_val=max_val, filter_size=filter_size, filter_sigma=filter_sigma, k1=k1, k2=k2)\n",
        "    mssim = np.append(mssim, ssim)\n",
        "    mcs = np.append(mcs, cs)\n",
        "\n",
        "    filtered = [convolve(im, downsample_filter, mode='reflect') for im in [im1, im2]]\n",
        "    im1, im2 = [x[:, ::2, ::2, :] for x in filtered]\n",
        "\n",
        "  return np.prod(mcs[0:levels-1] ** weights[0:levels-1]) * (mssim[levels-1] ** weights[levels-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7HoKU-8xNS5"
      },
      "source": [
        "# Helper functions for losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF2xBEoBxT8f"
      },
      "source": [
        "#color loss helper functions\n",
        "\n",
        "#helper function for performing gaussian blur on an image\n",
        "def blur(x):\n",
        "\n",
        "  blurred_img = tfa.image.gaussian_filter2d(x,[21,21], 3)\n",
        "\n",
        "  return blurred_img\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "#total variation loss helper functions\n",
        "\n",
        "def tensor_size(tensor):\n",
        "  from operator import mul\n",
        "  #reduce syntax -> (operator, inputlist, initalizer)\n",
        "  return reduce(mul, (d.value for d in tensor.get_shape()[1:]), 1)\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "#psnr loss helper functions\n",
        "\n",
        "def log10(x):\n",
        "  numerator = tf.compat.v1.log(x)\n",
        "  denominator = tf.compat.v1.log(tf.constant(10, dtype=numerator.dtype))\n",
        "  return numerator / denominator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejPsOQnQv4KI"
      },
      "source": [
        "# Defining losses and training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDcE3-qcv8nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b66058b4-4c0e-462c-b6f0-43bb715d6374"
      },
      "source": [
        "with tf.Graph().as_default(), tf.compat.v1.Session() as sess:\n",
        "\n",
        "  phone_ = tf.compat.v1.placeholder(tf.float32, [None, PATCH_SIZE])\n",
        "  #reshaping the placeholder to match the shape of image\n",
        "  phone_image = tf.reshape(phone_, [-1, PATCH_HEIGHT, PATCH_WIDTH, 3])\n",
        "\n",
        "  dslr_ = tf.compat.v1.placeholder(tf.float32, [None, PATCH_SIZE])\n",
        "  #reshaping the placeholder to match the shape of image\n",
        "  dslr_image = tf.reshape(dslr_, [-1, PATCH_HEIGHT, PATCH_WIDTH, 3])\n",
        "\n",
        "\n",
        "  adv_ = tf.compat.v1.placeholder(tf.float32, [None, 1])\n",
        "  \n",
        "  #get processed enhanced image from the model\n",
        "  enhanced = residualnetwork(phone_image)\n",
        "\n",
        "  #transform both dslr and enhanced images to grayscale (in order to target specifically on texture processing)\n",
        "  enhanced_gray = tf.reshape(tf.image.rgb_to_grayscale(enhanced), [-1, PATCH_WIDTH * PATCH_HEIGHT])\n",
        "  dslr_gray = tf.reshape(tf.image.rgb_to_grayscale(dslr_image),[-1, PATCH_WIDTH * PATCH_HEIGHT])\n",
        "\n",
        "  #push randomly the enhanced or dslr image to an adversarial CNN - discriminator\n",
        "  adversarial_ = tf.multiply(enhanced_gray, 1 - adv_) + tf.multiply(dslr_gray, adv_)\n",
        "  adversarial_image = tf.reshape(adversarial_, [-1, PATCH_HEIGHT, PATCH_WIDTH, 1]) #here the value is 1 because it is a grayscale image\n",
        "\n",
        "  discrim_predictions = adverserialnetwork(adversarial_image)\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "  #1)texture (adversarial) loss\n",
        "\n",
        "  #concatenates or joins two tensors\n",
        "  #https://www.tensorflow.org/api_docs/python/tf/concat\n",
        "  #here 1 denotes the axis. 1 means x axis, 0 means y axis\n",
        "\n",
        "  discrim_target = tf.concat([adv_, 1 - adv_], 1)\n",
        "\n",
        "  #clip_by_value clips tensor values to a specified min and max\n",
        "  #SYNTAX -> tf.clip_by_value(t, clip_value_min, clip_value_max, name=None)\n",
        "\n",
        "  loss_discrim = -tf.reduce_sum(discrim_target * tf.math.log(tf.clip_by_value(discrim_predictions, 1e-10, 1.0)))\n",
        "  loss_texture = -loss_discrim\n",
        "\n",
        "  correct_predictions = tf.equal(tf.argmax(discrim_predictions, 1), tf.argmax(discrim_target, 1))\n",
        "  discim_accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "  #2)color loss\n",
        "\n",
        "  enhanced_image_blur = blur(enhanced)\n",
        "  dslr_image_blur = blur(dslr_image)\n",
        "\n",
        "  loss_color = tf.reduce_sum(tf.pow(dslr_image_blur - enhanced_image_blur, 2))/(2 * batch_size)\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "  #3)total variation loss\n",
        "  #Total variation loss is the sum of the absolute differences for neighboring pixel-values in the input images. \n",
        "  #This measures how much noise is in the images. Adding total variation loss to the training loss removes the rough \n",
        "  #texture of the image and the resultant image looks much smoother.\n",
        "  \n",
        "  #y -> vertical axis, x -> horizontal axis\n",
        "  batch_shape = (batch_size, PATCH_WIDTH, PATCH_HEIGHT, 3)\n",
        "\n",
        "  tv_y_size = tensor_size(enhanced[:,1:,:,:])\n",
        "  tv_x_size = tensor_size(enhanced[:,:,1:,:])\n",
        "  \n",
        "  #this is done to measure the difference between the values of the current pixel and the neighbouring pixel and find the l2 norm (output = sum(t ** 2) / 2)\n",
        "  y_tv = tf.nn.l2_loss(enhanced[:,1:,:,:] - enhanced[:,:batch_shape[1]-1,:,:])\n",
        "  x_tv = tf.nn.l2_loss(enhanced[:,:,1:,:] - enhanced[:,:,:batch_shape[2]-1,:])\n",
        "  \n",
        "  loss_tv = 2 * (x_tv/tv_x_size + y_tv/tv_y_size) / batch_size\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "  #4)content loss (using pre-trained VGG-19 network)\n",
        "\n",
        "  CONTENT_LAYER = 'relu5_4'\n",
        "\n",
        "  enhanced_vgg = net(vgg_dir, preprocess(enhanced * 255))\n",
        "  dslr_vgg = net(vgg_dir, preprocess(dslr_image * 255))\n",
        "\n",
        "  content_size = tensor_size(dslr_vgg[CONTENT_LAYER]) * batch_size\n",
        "  loss_content = 2 * tf.nn.l2_loss(enhanced_vgg[CONTENT_LAYER] - dslr_vgg[CONTENT_LAYER]) / content_size\n",
        "  \n",
        "#------------------------------------------------------------------\n",
        "\n",
        "  #final loss\n",
        "  \n",
        "  loss_generator = w_content * loss_content + w_texture * loss_texture + w_color * loss_color + w_tv * loss_tv\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "  #psnr loss (PSNR - peak signal to noise ratio)\n",
        "  #Peak signal-to-noise ratio definition (PSNR) is most commonly used as a quality estimation for the loss of quality through \n",
        "  #different codecs and image compression where the signal is the original image and the noise is error created by compressing the image.\n",
        "\n",
        "  #PSNR is very common for evaluating image enhancement techniques, such as Super resolution where the signal is the original/ground truth \n",
        "  #image and the noise is the error not recovered by the model.\n",
        "\n",
        "  #Reference - https://towardsdatascience.com/deep-learning-image-enhancement-insights-on-loss-function-engineering-f57ccbb585d7#:~:text=Peak%20signal%2Dto%2Dnoise%20ratio%20definition%20(PSNR)%20is,created%20by%20compressing%20the%20image.\n",
        "\n",
        "  enhanced_flat = tf.reshape(enhanced, [-1, PATCH_SIZE])\n",
        "\n",
        "  loss_mse = tf.reduce_sum(tf.pow(dslr_ - enhanced_flat, 2))/(PATCH_SIZE * batch_size)\n",
        "  loss_psnr = 20 * log10(1.0 / tf.sqrt(loss_mse))\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "  #TRAINING THE MODEL\n",
        "\n",
        "  #optimize parameters of image enhancement (generator) and discriminator networks\n",
        "\n",
        "  generator_vars = [v for v in tf.compat.v1.global_variables() if v.name.startswith(\"generator\")]\n",
        "  discriminator_vars = [v for v in tf.compat.v1.global_variables() if v.name.startswith(\"discriminator\")]\n",
        "\n",
        "  #training using Adam Optimizer as optimizer function to reduce the generator and discriminator loss\n",
        "  train_step_gen = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss_generator, var_list=generator_vars)\n",
        "  train_step_disc = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss_discrim, var_list=discriminator_vars)\n",
        "\n",
        "  saver = tf.compat.v1.train.Saver(var_list=generator_vars, max_to_keep=100)\n",
        "\n",
        "  #code to restore model\n",
        "  ckpt = tf.compat.v1.train.get_checkpoint_state(os.path.dirname('drive/MyDrive/Innovation_Practices_Lab/models/checkpoint'))\n",
        "\n",
        "  # if that checkpoint exists, restore from checkpoint\n",
        "  if ckpt and ckpt.model_checkpoint_path:\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "    #if some variables, we must initialize those variables\n",
        "    global_vars = tf.compat.v1.global_variables()\n",
        "    is_not_initialized = sess.run([tf.compat.v1.is_variable_initialized(var) for var in global_vars])\n",
        "    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
        "\n",
        "    # for i in not_initialized_vars: # only for testing\n",
        "    #   print(i.name)\n",
        "\n",
        "    if len(not_initialized_vars):\n",
        "      sess.run(tf.compat.v1.variables_initializer(not_initialized_vars))\n",
        "\n",
        "  else:\n",
        "    print('Initializing all variables from start')\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "  #loading training and test data\n",
        "\n",
        "  print(\"Loading test data...\")\n",
        "  test_data, test_answ = load_test_data(phone, dped_dir, PATCH_SIZE)\n",
        "  print(\"Test data was loaded\\n\")\n",
        "\n",
        "  print(\"Loading training data...\")\n",
        "  train_data, train_answ = load_batch(phone, dped_dir, train_size, PATCH_SIZE)\n",
        "  print(\"Training data was loaded\\n\")\n",
        "\n",
        "  TEST_SIZE = test_data.shape[0]\n",
        "  num_test_batches = int(test_data.shape[0] / batch_size)\n",
        "\n",
        "  print('Training network')\n",
        "\n",
        "  #variables to store losses of generator and discriminator while training\n",
        "  train_loss_gen = 0.0\n",
        "  train_acc_discrim = 0.0\n",
        "\n",
        "  #other variables\n",
        "  all_zeros = np.reshape(np.zeros((batch_size, 1)), [batch_size, 1])\n",
        "  test_crops = test_data[np.random.randint(0, TEST_SIZE, 5), :]\n",
        "\n",
        "  #creating log file\n",
        "  logs = open('drive/MyDrive/Innovation_Practices_Lab/model_logs/' + phone + '.txt', \"w+\")\n",
        "  logs.close()\n",
        "\n",
        "  for i in range(num_train_iters):\n",
        "\n",
        "    #training the generator\n",
        "\n",
        "    idx_train = np.random.randint(0, train_size, batch_size) #selecting batch_size random elements from size 0 to train_size\n",
        "\n",
        "    phone_images = train_data[idx_train]\n",
        "    dslr_images = train_answ[idx_train]\n",
        "\n",
        "    #setting the placeholders with the values and running the session\n",
        "    [loss_temp, temp] = sess.run([loss_generator, train_step_gen],feed_dict={phone_: phone_images, dslr_: dslr_images, adv_: all_zeros})\n",
        "    train_loss_gen += loss_temp / eval_step\n",
        "\n",
        "    #train discriminator\n",
        "\n",
        "    idx_train = np.random.randint(0, train_size, batch_size)\n",
        "\n",
        "    #generate image swaps (dslr or enhanced) for discriminator so that either the enhanced image or dslr image goes into the discriminator\n",
        "    swaps = np.reshape(np.random.randint(0, 2, batch_size), [batch_size, 1])\n",
        "\n",
        "    phone_images = train_data[idx_train]\n",
        "    dslr_images = train_answ[idx_train]\n",
        "\n",
        "    [accuracy_temp, temp] = sess.run([discim_accuracy, train_step_disc], feed_dict={phone_: phone_images, dslr_: dslr_images, adv_: swaps})\n",
        "    train_acc_discrim += accuracy_temp / eval_step\n",
        "\n",
        "    if i % eval_step == 0:\n",
        "\n",
        "      # test generator and discriminator CNNs\n",
        "\n",
        "      test_losses_gen = np.zeros((1, 6))\n",
        "      test_accuracy_disc = 0.0\n",
        "      loss_ssim = 0.0\n",
        "\n",
        "      for j in range(num_test_batches):\n",
        "\n",
        "        be = j * batch_size\n",
        "        en = (j+1) * batch_size\n",
        "\n",
        "        swaps = np.reshape(np.random.randint(0, 2, batch_size), [batch_size, 1])\n",
        "\n",
        "        phone_images = test_data[be:en]\n",
        "        dslr_images = test_answ[be:en]\n",
        "\n",
        "        [enhanced_crops, accuracy_disc, losses] = sess.run([enhanced, discim_accuracy, \\\n",
        "        [loss_generator, loss_content, loss_color, loss_texture, loss_tv, loss_psnr]], \\\n",
        "        feed_dict={phone_: phone_images, dslr_: dslr_images, adv_: swaps})\n",
        "\n",
        "        test_losses_gen += np.asarray(losses) / num_test_batches\n",
        "        test_accuracy_disc += accuracy_disc / num_test_batches\n",
        "\n",
        "        loss_ssim += MultiScaleSSIM(np.reshape(dslr_images * 255, [batch_size, PATCH_HEIGHT, PATCH_WIDTH, 3]), enhanced_crops * 255) / num_test_batches\n",
        "\n",
        "      logs_disc = \"step %d, %s | discriminator accuracy | train: %.4g, test: %.4g\" % \\\n",
        "      (i, phone, train_acc_discrim, test_accuracy_disc)\n",
        "\n",
        "      logs_gen = \"generator losses | train: %.4g, test: %.4g | content: %.4g, color: %.4g, texture: %.4g, tv: %.4g | psnr: %.4g, ms-ssim: %.4g\\n\" % \\\n",
        "      (train_loss_gen, test_losses_gen[0][0], test_losses_gen[0][1], test_losses_gen[0][2],\n",
        "      test_losses_gen[0][3], test_losses_gen[0][4], test_losses_gen[0][5], loss_ssim)\n",
        "\n",
        "      print(logs_disc)\n",
        "      print(logs_gen)\n",
        "\n",
        "      #save the results to log file\n",
        "\n",
        "      logs = open('drive/MyDrive/Innovation_Practices_Lab/model_logs/' + phone + '.txt', \"a\")\n",
        "      logs.write(logs_disc)\n",
        "      logs.write('\\n')\n",
        "      logs.write(logs_gen)\n",
        "      logs.write('\\n')\n",
        "      logs.close()\n",
        "\n",
        "      #save visual results for several test image crops\n",
        "\n",
        "      enhanced_crops = sess.run(enhanced, feed_dict={phone_: test_crops, dslr_: dslr_images, adv_: all_zeros})\n",
        "\n",
        "      idx = 0\n",
        "      \n",
        "      for crop in enhanced_crops:\n",
        "        before_after = np.hstack((np.reshape(test_crops[idx], [PATCH_HEIGHT, PATCH_WIDTH, 3]), crop))\n",
        "        imageio.imwrite('drive/MyDrive/Innovation_Practices_Lab/results/' + str(phone)+ \"_\" + str(idx) + '_iteration_' + str(i) + '.jpg', before_after)\n",
        "        idx += 1\n",
        "\n",
        "      train_loss_gen = 0.0\n",
        "      train_acc_discrim = 0.0\n",
        "\n",
        "      #save the model that corresponds to the current iteration\n",
        "\n",
        "      save_path = saver.save(sess, 'drive/MyDrive/Innovation_Practices_Lab/models/' + str(phone) + '_iteration_' + str(i) + '.ckpt', write_meta_graph=False)\n",
        "      print(\"Saved model in \" + save_path)\n",
        "\n",
        "      #reload a different batch of training data\n",
        "\n",
        "      del train_data\n",
        "      del train_answ\n",
        "      train_data, train_answ = load_batch(phone, dped_dir, train_size, PATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"discriminator/Softmax:0\", shape=(?, 2), dtype=float32)\n",
            "Loading test data...\n",
            "0% done\n",
            "2% done\n",
            "5% done\n",
            "7% done\n",
            "9% done\n",
            "12% done\n",
            "14% done\n",
            "16% done\n",
            "19% done\n",
            "21% done\n",
            "23% done\n",
            "26% done\n",
            "28% done\n",
            "30% done\n",
            "33% done\n",
            "35% done\n",
            "37% done\n",
            "40% done\n",
            "42% done\n",
            "44% done\n",
            "47% done\n",
            "49% done\n",
            "51% done\n",
            "53% done\n",
            "56% done\n",
            "58% done\n",
            "60% done\n",
            "63% done\n",
            "65% done\n",
            "67% done\n",
            "70% done\n",
            "72% done\n",
            "74% done\n",
            "77% done\n",
            "79% done\n",
            "81% done\n",
            "84% done\n",
            "86% done\n",
            "88% done\n",
            "91% done\n",
            "93% done\n",
            "95% done\n",
            "98% done\n",
            "Test data was loaded\n",
            "\n",
            "Loading training data...\n",
            "2% done\n",
            "3% done\n",
            "5% done\n",
            "7% done\n",
            "8% done\n",
            "10% done\n",
            "12% done\n",
            "13% done\n",
            "15% done\n",
            "17% done\n",
            "18% done\n",
            "20% done\n",
            "22% done\n",
            "23% done\n",
            "25% done\n",
            "27% done\n",
            "28% done\n",
            "30% done\n",
            "32% done\n",
            "33% done\n",
            "35% done\n",
            "37% done\n",
            "38% done\n",
            "40% done\n",
            "42% done\n",
            "43% done\n",
            "45% done\n",
            "47% done\n",
            "48% done\n",
            "50% done\n",
            "52% done\n",
            "53% done\n",
            "55% done\n",
            "57% done\n",
            "58% done\n",
            "60% done\n",
            "62% done\n",
            "63% done\n",
            "65% done\n",
            "67% done\n",
            "68% done\n",
            "70% done\n",
            "72% done\n",
            "73% done\n",
            "75% done\n",
            "77% done\n",
            "78% done\n",
            "80% done\n",
            "82% done\n",
            "83% done\n",
            "85% done\n",
            "87% done\n",
            "88% done\n",
            "90% done\n",
            "92% done\n",
            "93% done\n",
            "95% done\n",
            "97% done\n",
            "98% done\n",
            "100% done\n",
            "Training data was loaded\n",
            "\n",
            "Training network\n",
            "step 0, iphone | discriminator accuracy | train: 0.0006, test: 0.6819\n",
            "generator losses | train: 0.6273, test: 956.7 | content: 18.64, color: 1600, texture: -30.63, tv: 0.0003291 | psnr: 9.419, ms-ssim: 0.4822\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.005461573600769043, 0.7700662612915039]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.00518953800201416, 0.7391876578330994]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.0541229248046875, 0.91357421875]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved model in drive/MyDrive/Innovation_Practices_Lab/models/iphone_iteration_0.ckpt\n",
            "2% done\n",
            "3% done\n",
            "5% done\n",
            "7% done\n",
            "8% done\n",
            "10% done\n",
            "12% done\n",
            "13% done\n",
            "15% done\n",
            "17% done\n",
            "18% done\n",
            "20% done\n",
            "22% done\n",
            "23% done\n",
            "25% done\n",
            "27% done\n",
            "28% done\n",
            "30% done\n",
            "32% done\n",
            "33% done\n",
            "35% done\n",
            "37% done\n",
            "38% done\n",
            "40% done\n",
            "42% done\n",
            "43% done\n",
            "45% done\n",
            "47% done\n",
            "48% done\n",
            "50% done\n",
            "52% done\n",
            "53% done\n",
            "55% done\n",
            "57% done\n",
            "58% done\n",
            "60% done\n",
            "62% done\n",
            "63% done\n",
            "65% done\n",
            "67% done\n",
            "68% done\n",
            "70% done\n",
            "72% done\n",
            "73% done\n",
            "75% done\n",
            "77% done\n",
            "78% done\n",
            "80% done\n",
            "82% done\n",
            "83% done\n",
            "85% done\n",
            "87% done\n",
            "88% done\n",
            "90% done\n",
            "92% done\n",
            "93% done\n",
            "95% done\n",
            "97% done\n",
            "98% done\n",
            "100% done\n",
            "step 1000, iphone | discriminator accuracy | train: 0.5094, test: 0.5074\n",
            "generator losses | train: 153.4, test: 111.9 | content: 8.53, color: 115.2, texture: -35.21, tv: 0.002101 | psnr: 19.97, ms-ssim: 0.9074\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved model in drive/MyDrive/Innovation_Practices_Lab/models/iphone_iteration_1000.ckpt\n",
            "2% done\n",
            "3% done\n",
            "5% done\n",
            "7% done\n",
            "8% done\n",
            "10% done\n",
            "12% done\n",
            "13% done\n",
            "15% done\n",
            "17% done\n",
            "18% done\n",
            "20% done\n",
            "22% done\n",
            "23% done\n",
            "25% done\n",
            "27% done\n",
            "28% done\n",
            "30% done\n",
            "32% done\n",
            "33% done\n",
            "35% done\n",
            "37% done\n",
            "38% done\n",
            "40% done\n",
            "42% done\n",
            "43% done\n",
            "45% done\n",
            "47% done\n",
            "48% done\n",
            "50% done\n",
            "52% done\n",
            "53% done\n",
            "55% done\n",
            "57% done\n",
            "58% done\n",
            "60% done\n",
            "62% done\n",
            "63% done\n",
            "65% done\n",
            "67% done\n",
            "68% done\n",
            "70% done\n",
            "72% done\n",
            "73% done\n",
            "75% done\n",
            "77% done\n",
            "78% done\n",
            "80% done\n",
            "82% done\n",
            "83% done\n",
            "85% done\n",
            "87% done\n",
            "88% done\n",
            "90% done\n",
            "92% done\n",
            "93% done\n",
            "95% done\n",
            "97% done\n",
            "98% done\n",
            "100% done\n",
            "step 2000, iphone | discriminator accuracy | train: 0.5116, test: 0.5179\n",
            "generator losses | train: 107, test: 103.5 | content: 8.152, color: 108, texture: -35.82, tv: 0.00189 | psnr: 20.2, ms-ssim: 0.9097\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved model in drive/MyDrive/Innovation_Practices_Lab/models/iphone_iteration_2000.ckpt\n",
            "2% done\n",
            "3% done\n",
            "5% done\n",
            "7% done\n",
            "8% done\n",
            "10% done\n",
            "12% done\n",
            "13% done\n",
            "15% done\n",
            "17% done\n",
            "18% done\n",
            "20% done\n",
            "22% done\n",
            "23% done\n",
            "25% done\n",
            "27% done\n",
            "28% done\n",
            "30% done\n",
            "32% done\n",
            "33% done\n",
            "35% done\n",
            "37% done\n",
            "38% done\n",
            "40% done\n",
            "42% done\n",
            "43% done\n",
            "45% done\n",
            "47% done\n",
            "48% done\n",
            "50% done\n",
            "52% done\n",
            "53% done\n",
            "55% done\n",
            "57% done\n",
            "58% done\n",
            "60% done\n",
            "62% done\n",
            "63% done\n",
            "65% done\n",
            "67% done\n",
            "68% done\n",
            "70% done\n",
            "72% done\n",
            "73% done\n",
            "75% done\n",
            "77% done\n",
            "78% done\n",
            "80% done\n",
            "82% done\n",
            "83% done\n",
            "85% done\n",
            "87% done\n",
            "88% done\n",
            "90% done\n",
            "92% done\n",
            "93% done\n",
            "95% done\n",
            "97% done\n",
            "98% done\n",
            "100% done\n",
            "step 3000, iphone | discriminator accuracy | train: 0.5167, test: 0.503\n",
            "generator losses | train: 100.6, test: 103.2 | content: 7.77, color: 112.2, texture: -34.78, tv: 0.00212 | psnr: 20.09, ms-ssim: 0.9095\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved model in drive/MyDrive/Innovation_Practices_Lab/models/iphone_iteration_3000.ckpt\n",
            "2% done\n",
            "3% done\n",
            "5% done\n",
            "7% done\n",
            "8% done\n",
            "10% done\n",
            "12% done\n",
            "13% done\n",
            "15% done\n",
            "17% done\n",
            "18% done\n",
            "20% done\n",
            "22% done\n",
            "23% done\n",
            "25% done\n",
            "27% done\n",
            "28% done\n",
            "30% done\n",
            "32% done\n",
            "33% done\n",
            "35% done\n",
            "37% done\n",
            "38% done\n",
            "40% done\n",
            "42% done\n",
            "43% done\n",
            "45% done\n",
            "47% done\n",
            "48% done\n",
            "50% done\n",
            "52% done\n",
            "53% done\n",
            "55% done\n",
            "57% done\n",
            "58% done\n",
            "60% done\n",
            "62% done\n",
            "63% done\n",
            "65% done\n",
            "67% done\n",
            "68% done\n",
            "70% done\n",
            "72% done\n",
            "73% done\n",
            "75% done\n",
            "77% done\n",
            "78% done\n",
            "80% done\n",
            "82% done\n",
            "83% done\n",
            "85% done\n",
            "87% done\n",
            "88% done\n",
            "90% done\n",
            "92% done\n",
            "93% done\n",
            "95% done\n",
            "97% done\n",
            "98% done\n",
            "100% done\n",
            "step 4000, iphone | discriminator accuracy | train: 0.5184, test: 0.4963\n",
            "generator losses | train: 96.01, test: 103.3 | content: 7.68, color: 113, texture: -34.85, tv: 0.002416 | psnr: 20.11, ms-ssim: 0.9129\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.023676931858062744, 0.5623477101325989]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.03171473741531372, 0.3979284167289734]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved model in drive/MyDrive/Innovation_Practices_Lab/models/iphone_iteration_4000.ckpt\n",
            "2% done\n",
            "3% done\n",
            "5% done\n",
            "7% done\n",
            "8% done\n",
            "10% done\n",
            "12% done\n",
            "13% done\n",
            "15% done\n",
            "17% done\n",
            "18% done\n",
            "20% done\n",
            "22% done\n",
            "23% done\n",
            "25% done\n",
            "27% done\n",
            "28% done\n",
            "30% done\n",
            "32% done\n",
            "33% done\n",
            "35% done\n",
            "37% done\n",
            "38% done\n",
            "40% done\n",
            "42% done\n",
            "43% done\n",
            "45% done\n",
            "47% done\n",
            "48% done\n",
            "50% done\n",
            "52% done\n",
            "53% done\n",
            "55% done\n",
            "57% done\n",
            "58% done\n",
            "60% done\n",
            "62% done\n",
            "63% done\n",
            "65% done\n",
            "67% done\n",
            "68% done\n",
            "70% done\n",
            "72% done\n",
            "73% done\n",
            "75% done\n",
            "77% done\n",
            "78% done\n",
            "80% done\n",
            "82% done\n",
            "83% done\n",
            "85% done\n",
            "87% done\n",
            "88% done\n",
            "90% done\n",
            "92% done\n",
            "93% done\n",
            "95% done\n",
            "97% done\n",
            "98% done\n",
            "100% done\n",
            "step 5000, iphone | discriminator accuracy | train: 0.5241, test: 0.4849\n",
            "generator losses | train: 90.73, test: 105.4 | content: 7.676, color: 118.8, texture: -35.44, tv: 0.002328 | psnr: 19.96, ms-ssim: 0.9132\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.014442205429077148, 0.5464882254600525]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved model in drive/MyDrive/Innovation_Practices_Lab/models/iphone_iteration_5000.ckpt\n",
            "2% done\n",
            "3% done\n",
            "5% done\n",
            "7% done\n",
            "8% done\n",
            "10% done\n",
            "12% done\n",
            "13% done\n",
            "15% done\n",
            "17% done\n",
            "18% done\n",
            "20% done\n",
            "22% done\n",
            "23% done\n",
            "25% done\n",
            "27% done\n",
            "28% done\n",
            "30% done\n",
            "32% done\n",
            "33% done\n",
            "35% done\n",
            "37% done\n",
            "38% done\n",
            "40% done\n",
            "42% done\n",
            "43% done\n",
            "45% done\n",
            "47% done\n",
            "48% done\n",
            "50% done\n",
            "52% done\n",
            "53% done\n",
            "55% done\n",
            "57% done\n",
            "58% done\n",
            "60% done\n",
            "62% done\n",
            "63% done\n",
            "65% done\n",
            "67% done\n",
            "68% done\n",
            "70% done\n",
            "72% done\n",
            "73% done\n",
            "75% done\n",
            "77% done\n",
            "78% done\n",
            "80% done\n",
            "82% done\n",
            "83% done\n",
            "85% done\n",
            "87% done\n",
            "88% done\n",
            "90% done\n",
            "92% done\n",
            "93% done\n",
            "95% done\n",
            "97% done\n",
            "98% done\n",
            "100% done\n",
            "step 6000, iphone | discriminator accuracy | train: 0.5307, test: 0.4986\n",
            "generator losses | train: 88.48, test: 100.5 | content: 7.49, color: 111.9, texture: -35.15, tv: 0.002384 | psnr: 20.15, ms-ssim: 0.9127\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.020932555198669434, 0.5452311635017395]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved model in drive/MyDrive/Innovation_Practices_Lab/models/iphone_iteration_6000.ckpt\n",
            "2% done\n",
            "3% done\n",
            "5% done\n",
            "7% done\n",
            "8% done\n",
            "10% done\n",
            "12% done\n",
            "13% done\n",
            "15% done\n",
            "17% done\n",
            "18% done\n",
            "20% done\n",
            "22% done\n",
            "23% done\n",
            "25% done\n",
            "27% done\n",
            "28% done\n",
            "30% done\n",
            "32% done\n",
            "33% done\n",
            "35% done\n",
            "37% done\n",
            "38% done\n",
            "40% done\n",
            "42% done\n",
            "43% done\n",
            "45% done\n",
            "47% done\n",
            "48% done\n",
            "50% done\n",
            "52% done\n",
            "53% done\n",
            "55% done\n",
            "57% done\n",
            "58% done\n",
            "60% done\n",
            "62% done\n",
            "63% done\n",
            "65% done\n",
            "67% done\n",
            "68% done\n",
            "70% done\n",
            "72% done\n",
            "73% done\n",
            "75% done\n",
            "77% done\n",
            "78% done\n",
            "80% done\n",
            "82% done\n",
            "83% done\n",
            "85% done\n",
            "87% done\n",
            "88% done\n",
            "90% done\n",
            "92% done\n",
            "93% done\n",
            "95% done\n",
            "97% done\n",
            "98% done\n",
            "100% done\n",
            "step 7000, iphone | discriminator accuracy | train: 0.5299, test: 0.5391\n",
            "generator losses | train: 85.54, test: 105.3 | content: 7.362, color: 122.1, texture: -34.38, tv: 0.002492 | psnr: 19.83, ms-ssim: 0.9141\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.007979094982147217, 0.5068550109863281]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0.478515625, 1.0006483793258667]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved model in drive/MyDrive/Innovation_Practices_Lab/models/iphone_iteration_7000.ckpt\n",
            "2% done\n",
            "3% done\n",
            "5% done\n",
            "7% done\n",
            "8% done\n",
            "10% done\n",
            "12% done\n",
            "13% done\n",
            "15% done\n",
            "17% done\n",
            "18% done\n",
            "20% done\n",
            "22% done\n",
            "23% done\n",
            "25% done\n",
            "27% done\n",
            "28% done\n",
            "30% done\n",
            "32% done\n",
            "33% done\n",
            "35% done\n",
            "37% done\n",
            "38% done\n",
            "40% done\n",
            "42% done\n",
            "43% done\n",
            "45% done\n",
            "47% done\n",
            "48% done\n",
            "50% done\n",
            "52% done\n",
            "53% done\n",
            "55% done\n",
            "57% done\n",
            "58% done\n",
            "60% done\n",
            "62% done\n",
            "63% done\n",
            "65% done\n",
            "67% done\n",
            "68% done\n",
            "70% done\n",
            "72% done\n",
            "73% done\n",
            "75% done\n",
            "77% done\n",
            "78% done\n",
            "80% done\n",
            "82% done\n",
            "83% done\n",
            "85% done\n",
            "87% done\n",
            "88% done\n",
            "90% done\n",
            "92% done\n",
            "93% done\n",
            "95% done\n",
            "97% done\n",
            "98% done\n",
            "100% done\n",
            "step 8000, iphone | discriminator accuracy | train: 0.5361, test: 0.4909\n",
            "generator losses | train: 81.3, test: 109.3 | content: 7.342, color: 131.9, texture: -34.96, tv: 0.002458 | psnr: 19.62, ms-ssim: 0.9148\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.004300177097320557, 0.5397264957427979]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.029374778270721436, 0.39109402894973755]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved model in drive/MyDrive/Innovation_Practices_Lab/models/iphone_iteration_8000.ckpt\n",
            "2% done\n",
            "3% done\n",
            "5% done\n",
            "7% done\n",
            "8% done\n",
            "10% done\n",
            "12% done\n",
            "13% done\n",
            "15% done\n",
            "17% done\n",
            "18% done\n",
            "20% done\n",
            "22% done\n",
            "23% done\n",
            "25% done\n",
            "27% done\n",
            "28% done\n",
            "30% done\n",
            "32% done\n",
            "33% done\n",
            "35% done\n",
            "37% done\n",
            "38% done\n",
            "40% done\n",
            "42% done\n",
            "43% done\n",
            "45% done\n",
            "47% done\n",
            "48% done\n",
            "50% done\n",
            "52% done\n",
            "53% done\n",
            "55% done\n",
            "57% done\n",
            "58% done\n",
            "60% done\n",
            "62% done\n",
            "63% done\n",
            "65% done\n",
            "67% done\n",
            "68% done\n",
            "70% done\n",
            "72% done\n",
            "73% done\n",
            "75% done\n",
            "77% done\n",
            "78% done\n",
            "80% done\n",
            "82% done\n",
            "83% done\n",
            "85% done\n",
            "87% done\n",
            "88% done\n",
            "90% done\n",
            "92% done\n",
            "93% done\n",
            "95% done\n",
            "97% done\n",
            "98% done\n",
            "100% done\n",
            "step 9000, iphone | discriminator accuracy | train: 0.537, test: 0.5386\n",
            "generator losses | train: 79.02, test: 98.03 | content: 7.315, color: 108.8, texture: -34.45, tv: 0.00247 | psnr: 20.23, ms-ssim: 0.9161\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.004417359828948975, 0.5670396685600281]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.02683854103088379, 0.3753599524497986]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved model in drive/MyDrive/Innovation_Practices_Lab/models/iphone_iteration_9000.ckpt\n",
            "2% done\n",
            "3% done\n",
            "5% done\n",
            "7% done\n",
            "8% done\n",
            "10% done\n",
            "12% done\n",
            "13% done\n",
            "15% done\n",
            "17% done\n",
            "18% done\n",
            "20% done\n",
            "22% done\n",
            "23% done\n",
            "25% done\n",
            "27% done\n",
            "28% done\n",
            "30% done\n",
            "32% done\n",
            "33% done\n",
            "35% done\n",
            "37% done\n",
            "38% done\n",
            "40% done\n",
            "42% done\n",
            "43% done\n",
            "45% done\n",
            "47% done\n",
            "48% done\n",
            "50% done\n",
            "52% done\n",
            "53% done\n",
            "55% done\n",
            "57% done\n",
            "58% done\n",
            "60% done\n",
            "62% done\n",
            "63% done\n",
            "65% done\n",
            "67% done\n",
            "68% done\n",
            "70% done\n",
            "72% done\n",
            "73% done\n",
            "75% done\n",
            "77% done\n",
            "78% done\n",
            "80% done\n",
            "82% done\n",
            "83% done\n",
            "85% done\n",
            "87% done\n",
            "88% done\n",
            "90% done\n",
            "92% done\n",
            "93% done\n",
            "95% done\n",
            "97% done\n",
            "98% done\n",
            "100% done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4kudgKpQ98Q"
      },
      "source": [
        "# Helper functions for testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJdnxMzkRCfy"
      },
      "source": [
        "def get_resolutions():\n",
        "\n",
        "  #IMAGE_HEIGHT, IMAGE_WIDTH\n",
        "  res_sizes = {}\n",
        "\n",
        "  res_sizes[\"iphone\"] = [1536, 2048]\n",
        "  res_sizes[\"iphone_orig\"] = [1536, 2048]\n",
        "  res_sizes[\"blackberry\"] = [1560, 2080]\n",
        "  res_sizes[\"blackberry_orig\"] = [1560, 2080]\n",
        "  res_sizes[\"sony\"] = [1944, 2592]\n",
        "  res_sizes[\"sony_orig\"] = [1944, 2592]\n",
        "  res_sizes[\"high\"] = [1260, 1680]\n",
        "  res_sizes[\"medium\"] = [1024, 1366]\n",
        "  res_sizes[\"small\"] = [768, 1024]\n",
        "  res_sizes[\"tiny\"] = [600, 800]\n",
        "\n",
        "  return res_sizes\n",
        "\n",
        "\n",
        "def get_specified_res(res_sizes, phone, resolution):\n",
        "\n",
        "  if resolution == \"orig\":\n",
        "    IMAGE_HEIGHT = res_sizes[phone][0]\n",
        "    IMAGE_WIDTH = res_sizes[phone][1]\n",
        "  else:\n",
        "    IMAGE_HEIGHT = res_sizes[resolution][0]\n",
        "    IMAGE_WIDTH = res_sizes[resolution][1]\n",
        "\n",
        "  IMAGE_SIZE = IMAGE_WIDTH * IMAGE_HEIGHT * 3\n",
        "\n",
        "  return IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqSisXu8d-01"
      },
      "source": [
        "# Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtOQSqOleEO1"
      },
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "\n",
        "  #get all available image resolutions\n",
        "  res_sizes = get_resolutions()\n",
        "\n",
        "  #get the specified image resolution\n",
        "  IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_SIZE = get_specified_res(res_sizes, phone, resolution)\n",
        "\n",
        "  # create placeholders for input images\n",
        "  x_ = tf.compat.v1.placeholder(tf.float32, [None, IMAGE_SIZE])\n",
        "  x_image = tf.reshape(x_, [-1, IMAGE_HEIGHT, IMAGE_WIDTH, 3])\n",
        "\n",
        "  #generate enhanced image\n",
        "  enhanced = residualnetwork(x_image)  \n",
        "\n",
        "  test_dir = dped_dir + phone.replace(\"_orig\", \"\") + \"/test_data/full_size_test_images/\"\n",
        "  test_photos = [f for f in os.listdir(test_dir) if os.path.isfile(test_dir + f)]\n",
        "\n",
        "  if test_subset == \"small\":\n",
        "    #use first five images only\n",
        "    test_photos = test_photos[0:5]\n",
        "\n",
        "  iteration = np.array([9000]) #taking few iterations into account to see the learning of the model\n",
        "\n",
        "  for i in iteration:\n",
        "\n",
        "    #load pre-trained model\n",
        "    #reference 1 - https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model\n",
        "    #reference 2 - https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/import_meta_graph\n",
        "    \n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "    saver.restore(sess, \"drive/MyDrive/Innovation_Practices_Lab/models/\" + phone + \"_iteration_\" + str(i) +\".ckpt\")\n",
        "\n",
        "    for photo in test_photos:\n",
        "\n",
        "      #load training image and crop it if necessary\n",
        "\n",
        "      print(\"iteration \" + str(i) + \", processing image \" + photo)\n",
        "      image = np.float16(np.array(Image.fromarray(imageio.imread(test_dir + photo)).resize([res_sizes[phone][1], res_sizes[phone][0]]))) / 255\n",
        "\n",
        "      #image_crop = extract_crop(image, resolution, phone, res_sizes)\n",
        "      image_crop_2d = np.reshape(image, [1, IMAGE_SIZE])\n",
        "\n",
        "      #get enhanced image\n",
        "\n",
        "      enhanced_2d = sess.run(enhanced, feed_dict={x_: image_crop_2d})\n",
        "      enhanced_image = np.reshape(enhanced_2d, [IMAGE_HEIGHT, IMAGE_WIDTH, 3])\n",
        "\n",
        "      before_after = np.hstack((image, enhanced_image))\n",
        "      photo_name = photo.rsplit(\".\", 1)[0]\n",
        "\n",
        "      #save the results as .png images\n",
        "\n",
        "      #imageio.imwrite(\"drive/MyDrive/Innovation_Practices_Lab/visual_results/\" + phone + \"_\" + photo_name + \"_iteration_\" + str(i) + \"_enhanced.png\", enhanced_image)\n",
        "      imageio.imwrite(\"drive/MyDrive/Innovation_Practices_Lab/visual_results/\" + phone + \"_\" + photo_name + \"_iteration_\" + str(i) + \"_before_after.png\", before_after)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX2bPKmNhRri"
      },
      "source": [
        "# Hosting on Web"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4RMBnx7hYAD"
      },
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "def enhance_image(input_image, height, width):\n",
        "\n",
        "  with tf.compat.v1.Session() as sess:\n",
        "\n",
        "    #get the specified image resolution\n",
        "    if height > 2000 or width > 2000:\n",
        "      IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_SIZE = height // 2, width // 2, (height // 2)*(width // 2)*3\n",
        "\n",
        "    else:  \n",
        "      IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_SIZE = height, width, height*width*3\n",
        "\n",
        "    # create placeholders for input images\n",
        "    x_ = tf.compat.v1.placeholder(tf.float32, [None, IMAGE_SIZE])\n",
        "    x_image = tf.reshape(x_, [-1, IMAGE_HEIGHT, IMAGE_WIDTH, 3])\n",
        "\n",
        "    #generate enhanced image\n",
        "    enhanced = residualnetwork(x_image)\n",
        "\n",
        "    #load pre-trained model\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "    saver.restore(sess, \"drive/MyDrive/Innovation_Practices_Lab/models/iphone_iteration_9000.ckpt\")\n",
        "    #orig model - saver.restore(sess, \"drive/MyDrive/Innovation_Practices_Lab/models/original_models/iphone/iphone_orig\")\n",
        "\n",
        "    image = np.float32(np.array(input_image.resize([IMAGE_WIDTH, IMAGE_HEIGHT]))) / 255\n",
        "    image_crop_2d = np.reshape(image, [1, IMAGE_SIZE])\n",
        "\n",
        "    #get enhanced image\n",
        "\n",
        "    enhanced_2d = sess.run(enhanced, feed_dict={x_: image_crop_2d})\n",
        "    enhanced_image = np.reshape(enhanced_2d, [IMAGE_HEIGHT, IMAGE_WIDTH, 3])\n",
        "\n",
        "    #save the results as .png images\n",
        "\n",
        "    before_after = np.hstack((image, enhanced_image))\n",
        "\n",
        "    imageio.imwrite(\"drive/MyDrive/Innovation_Practices_Lab/visual_results/enhanced_image_before_after.png\", before_after)\n",
        "\n",
        "    new_enhanced_image = cv2.normalize(enhanced_image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
        "\n",
        "    img = Image.fromarray(new_enhanced_image)\n",
        "    rawBytes = io.BytesIO()\n",
        "    img.save(rawBytes, \"JPEG\")\n",
        "    rawBytes.seek(0)\n",
        "    img_base64 = base64.b64encode(rawBytes.read())\n",
        "\n",
        "    return jsonify({'status':str(img_base64)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp7HdYu656sh"
      },
      "source": [
        "!pip install flask\n",
        "!pip install flask-ngrok\n",
        "\n",
        "import flask\n",
        "import os,io,sys\n",
        "from flask import Flask, render_template , request , jsonify\n",
        "import base64\n",
        "\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__, template_folder='./drive/MyDrive/Innovation_Practices_Lab/Flask_web/templates', static_folder='./drive/MyDrive/Innovation_Practices_Lab/Flask_web/static')\n",
        "run_with_ngrok(app)   #starts ngrok when the app is run\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "  return render_template('index.html')\n",
        "\n",
        "@app.route('/test' , methods=['GET','POST'])\n",
        "def test():\n",
        "\tprint(\"log: got at test\" , file=sys.stderr)\n",
        "\treturn jsonify({'status':'success'})\n",
        "  \n",
        "@app.route('/predict', methods=['GET','POST'])\n",
        "def predict():\n",
        "  file = request.files['image'].read()\n",
        "  npimg = np.frombuffer(file,np.uint8)\n",
        "  input_image = cv2.imdecode(npimg,cv2.IMREAD_COLOR) #OpenCV uses BGR\n",
        "  input_image = cv2.cvtColor(input_image , cv2.COLOR_BGR2RGB)\n",
        "  response = enhance_image(Image.fromarray(input_image),input_image.shape[0], input_image.shape[1])\n",
        "  return response\n",
        "\n",
        "@app.after_request\n",
        "def after_request(response):\n",
        "  print(\"log: setting cors\" , file = sys.stderr)\n",
        "  response.headers.add('Access-Control-Allow-Origin', '*')\n",
        "  response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')\n",
        "  response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE')\n",
        "  \n",
        "  return response\n",
        "\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6sycJMGfkUa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}